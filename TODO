stat:
 - add counters to detect runaway epoll events (e.g., writable notifications when there is no data to send)
 - put detection of remote close in epoll handler and call handleRemoteClose()?  audit how client/server socket handle 0 reads.  Should those also call remote close?  Or just put the remote close logic only there?
        [1613321028:18967:ERR] [orig-server:127.0.0.1:35641>127.0.0.1:37400,578] missing 1373526 bytes from 1373526 byte response body [/root/project/http/origin/source/ESHttpOriginHandler.cpp:92]
        [1613321028:18967:CRT] [Aborted: 1/14]: gsignal
        [1613321028:18967:CRT] [Aborted: 2/14]: abort
        [1613321028:18967:CRT] [Aborted: 3/14]: /lib/x86_64-linux-gnu/libc.so.6(+0x3048a) [0x7ff2a107548a]
        [1613321028:18967:CRT] [Aborted: 4/14]: /lib/x86_64-linux-gnu/libc.so.6(+0x30502) [0x7ff2a1075502]
        [1613321028:18967:CRT] [Aborted: 5/14]: ES::HttpOriginHandler::consumeRequestBody(ES::HttpMultiplexer&, ES::HttpServerStream&, unsigned char const*, unsigned int, unsigned int*)
        [1613321028:18967:CRT] [Aborted: 6/14]: ES::HttpServerSocket::stateReceiveRequestBody(ES::HttpServerHandler&)
        [1613321028:18967:CRT] [Aborted: 7/14]: ES::HttpServerSocket::advanceStateMachine(ES::HttpServerHandler&, int)
        [1613321028:18967:CRT] [Aborted: 8/14]: ES::HttpServerSocket::handleReadable()
        [1613321028:18967:CRT] [Aborted: 9/14]: ESB::EpollMultiplexer::run(ESB::SharedInt*)
        [1613321028:18967:CRT] [Aborted:10/14]: ES::HttpProxyMultiplexer::run(ESB::SharedInt*)
        [1613321028:18967:CRT] [Aborted:11/14]: ESB::ThreadPoolWorker::run()
        [1613321028:18967:CRT] [Aborted:12/14]: ESB::Thread::ThreadEntry(void*)
        [1613321028:18967:CRT] [Aborted:13/14]: /lib/x86_64-linux-gnu/libpthread.so.0(+0x76db) [0x7ff2a185d6db]
        [1613321028:18967:CRT] [Aborted:14/14]: clone
 - Some change between de75ff65ea4c2ae866da5557c5da579b726c6759 and b6133cdecbb347431451c293fca62704f09c8bd0 reduced the connection reuse rate.  Check updated connection pool hash?
    de75ff65ea4c2ae866da5557c5da579b726c6759:
        [1595691738:14892:WRN] SERVER CONNECTION ACCEPTS: 448 [/home/jblatt/src/duderino/everscale/http/http1/source/ESHttpServerSimpleCounters.cpp:40]
        [1595691738:14892:WRN] SERVER AVG TRANS PER CONNECTION: N=448, MEAN=5591.52, VAR=4151887.43, MIN=1.00, MAX=7404.00 [/home/jblatt/src/duderino/everscale/base/source/ESBSharedAveragingCounter.cpp:33]
        [1595691738:14892:WRN] SERVER CONNECTION ACCEPTS: 504 [/home/jblatt/src/duderino/everscale/http/http1/source/ESHttpServerSimpleCounters.cpp:40]
        [1595691738:14892:WRN] SERVER AVG TRANS PER CONNECTION: N=504, MEAN=4970.24, VAR=112291.76, MIN=4585.00, MAX=5410.00 [/home/jblatt/src/duderino/everscale/base/source/ESBSharedAveragingCounter.cpp:33]
    b6133cdecbb347431451c293fca62704f09c8bd0:
        [1613339818:13448:WRN] SERVER CONNECTION ACCEPTS: 580 [/home/jblatt/src/duderino/everscale/http/http1/source/ESHttpServerSimpleCounters.cpp:40]
        [1613339818:13448:WRN] SERVER AVG TRANS PER CONNECTION: N=580, MEAN=4310.34, VAR=23385506.24, MIN=9.00, MAX=25907.00 [/home/jblatt/src/duderino/everscale/base/source/ESBSharedAveragingCounter.cpp:33]
        [1613339818:13448:WRN] SERVER CONNECTION ACCEPTS: 770 [/home/jblatt/src/duderino/everscale/http/http1/source/ESHttpServerSimpleCounters.cpp:40]
        [1613339818:13448:WRN] SERVER AVG TRANS PER CONNECTION: N=770, MEAN=3246.75, VAR=127045.33, MIN=2.00, MAX=3498.00 [/home/jblatt/src/duderino/everscale/base/source/ESBSharedAveragingCounter.cpp:33]
    - Connection reuse false test seems to be borked.  Does loadgen keep reusing the connection after close?  Clue:
        [1613340628:15905:WRN] [prox-server:127.0.0.1:38719>127.0.0.1:51460,1535] cannot send server response: Cleanup [/home/jblatt/src/duderino/everscale/http/proxy/source/ESHttpRoutingProxyHandler.cpp:167]
 - EpollMultiplexer dtor calls cleanup handlers on all active connections.  This returns them to the client/server socket factories instead of destroying them.
 - create WAN test using a special docker image with:  tc qdisc add dev lo root netem delay 100ms 20ms distribution normal
 - interop test with nodejs:  https://github.com/apache/trafficserver/pull/131/files#diff-7044deb3d91355abdd40d4f21f39a354ef0de8eb18d1f9d49e147cea927729b0
 - proxy handler should sanitize/drop/merge inbound request headers and outbound response headers
 - add test with really small socket buffers
 - Add test to connection pool test - server closes all connections while client connections are in pool.
 - ipv6 support
 - root metrics store
 - root config store

tls:
 - HTTP server should assert that client hostname matches server certificate (CN or SANs).
 - Consider option for HTTP server redirecting request if client hostname matches a more specific server certificate.
 - support removal of tls contexts from context index - overshadowed SANs should emerge.
 - evaluate against https://tools.ietf.org/html/rfc2818
 - revisit HttpClientSocket and HttpServerSocket wantRead and wantWrite:  allow the statemachine to finish the TLS handshake even when the stream is paused.
 - add secure/insecure bit to connected socket log address
 - use password protected private keys
 - server-side validation - verify issuer chain iff client cert is presented
 - Create integration test that exceeds chain length X client and server
 - Create integration test that presents cert for valid CN/valid SAN that is signed by a non-trusted CA
 - Create integration test that presents expired TTL
 - Create integration test that presents tampered cert
 - Create integration test that presents tampered private key
 - client side session resumption
 - server side session resumption
 - can boringssl lib's memory be reconciled with allocator framework?
 - enable ASLR
 - audit initialization for FIPS = https://boringssl.googlesource.com/boringssl/+/master/crypto/fipsmodule/FIPS.md and https://github.com/envoyproxy/envoy/commit/a734887ad06609cf0b3c023d38239bf3e79d3717
 - perfect forward secrecy
 - ocsp staples


scheduling:
 - hierarchical timing wheels to reduce ESB_OVERFLOW errors when scheduling timeouts beyond the timingwheel window.
 - add delay support (call me back later) with cancellation to epoll sockets and handlers

daemon:
 - Base class with preFork and postFork functions.  Subclass can bind to privileged ports in preFork
 - If configured, drop privs after postFork
 - Slurp keystore readable only by root into memory before dropping privs.  Use for private key password
 - start() takes over thread and waits for sigchild... restarting child processes that exit non-zero
 - catch SIGTERM and relay SIGTERM to child pid
 - maintain counters that track restarts
 - crash loop handling - exponential backoff with jitter?

proxy:
 - add a test for origin that sends a connection:close without a content-length and without a transfer-encoding.  Proxy should treat the rest of the stream as the body.
 - for proxy initiated connections, do not blindly relay connection close headers and content-length/transfer encoding headers from the client and server.
 - relay error codes to handler.endTransaction().  Instead of returning true|false in handleRead(), etc, return ESB::Error to the multiplexer and pass these back to the handlers.
 - move HttpServerHandler, HttpClientHandler, HttpMultiplexer and their deps into http-common.  Leave HttpMultiplexerExtended in http1
 - do not wait to receive entire request body before sending response body.  Instead stream them concurrently
 - add protocol coverage/interoperability tests:
   - (no request body/wait for socket close, unencoded request body / content length 0, chunked request body) X (client,server) X (request,response)
   - pipelining client that doesn't wait for the current transaction to complete before sending the next request
   - concurrent request/response streaming
   - correct handling when the server sends a response early... when shoudl it close the connection before finishing the response?
 - add stress tests:
   - proxy: backpressure works for clients that read response body slowly
   - proxy: backpressure works for servers that read request body slowly
   - server: mangled http client
   - client: mangled http server
 - Reduce proxy handler copying with a swapping rule (from recv to send):  swap iff send is empty and recv has no data from the next transaction in it.  Else copy
 - release send buffer when transitioning to parse headers.  acquire send buffer when transitioning to format headers (and later for bidy body streaming)
 - release recv buffer when transitioning to format headers.  acquire recv buffer whwn transitioning to parse headers (and later for bidi body streaming)

performance and efficiency:
 - do not unconditionally close server sockets after sending an error response - make that configurable and add a test for it
 - modify multiplexer - first write, then read.
 - Consider compacting recvBuffer unconditionally for every ServerSocket and ClientSocket handleReadable event.  This would do more memmoves in exchange for fewer socket recvs.
 - Consider using low watermarks for compacting and filling recvBuffer
 - Server socket:  release send and recv buffers in between transactions unless there's data for the next transaction in the app or socket recv buffs.
 - Consider always registering for read and write events and just discard events we don't care about / avoid a bunch of epollctl MOD calls.
 - Consider filling the recv buffer with data for the next transaction but not processing it until the current transaction completes.
 - compare performance of RELEASE vs RELEASENOPOOL vs. RELEASENOPOOL+tcmalloc on bare metal.  If allocators aren't the best option, limit their use to HttpTransaction and use standard new/delete/new[]/delete[] everywhere else.
 - try https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=70da268b569d32a9fddeea85dc18043de9d89f89
 - TCP tx and rx 0 copy
 - RSS and pinning threads to cores
 - ktls sockmap and splice
 - TFO
 - Experiment with larger userspace and TCP buffers
 - each multiplexer allocs array of max fd epoll events.  instead divide max fds by num multiplexers?  woudl ahve to ensure that no more than num max fds will be used in a multiplexer - don't accept new connections unless you have 2 fd free (1 fd for server connection, 1 fd for client connection)
 - hierarchical timing wheels to reduce memory usage for timers
 - wildcard indicies - support per-silo vs. global option (currently global only)
 - TLS_AES_128_GCM_SHA256
   - https://en.wikipedia.org/wiki/AES_instruction_set
   - https://en.wikipedia.org/wiki/Coffee_Lake
   - https://ark.intel.com/content/www/us/en/ark/products/191789/intel-core-i9-9900-processor-16m-cache-up-to-5-00-ghz.html
   - https://www.cyberciti.biz/faq/how-to-find-out-aes-ni-advanced-encryption-enabled-on-linux-system/

connection pool rework:
 - fix regression caused by switch from rb tree to hash table for connection pool.  https://docs.google.com/spreadsheets/d/1UOR-96YUOYD5poKrTPQv_GthBPWZb_fJzd6D4k0aNIk/edit#gid=2084099544
 - create option of global or per-silo connection pool and set default to per-silo.
 - enforce max connections per destination address (ip + port + transport + encrypted tuple)
 - evict old connections

code health:
 - Create macros for placement new / add noexcept and allocator references to all uses of placement new
 - Share more code between HttpClientSocket and HttpServerSocket
 - replace all these memcpy's of ESB::Buffer internals with a convenience function
 - Fewer log levels.  just error, warning, info, debug
 - Naming consistency:  pool to factory, create to acquire, destroy to release, handler to callback, getFoo to foo
 - use allocator cleanup handlers in embedded list test and more widely
 - use references instead of pointers when they shouldn't be NULL
 - reduce the number of name() and getName() functions
 - rework context name() composition/hierarchy.  Perhaps use variadic args in TCPConnectedSocket?

router:
 - create async router infra.  How can the async response be relayed?  callback function?
 - server trans routing:  resolve to VIP/hostname.  Use right after HTTP request headers have been received in ESHttpServerSocket.
 - client trans routing:  resolve to destination address.   Use in ESHttpClientSocketFactory::ExecuteClientTransaction()
 - make VIP/hostname first class member of transaction.  only routers can set it.  a naive router could set it to the host:port from the request URI
 - create simple DNS based async router impl.  Threadpool + synchronous DNS system resolver + local TTL and LRU-based cache.

load shedding:
 - pause/resume for http listening sockets (overloaded use case).  support PAUSE returns from HttpServerHandler::acceptConnection
 - load shedding dimensions - inbound connections, outbound connections, memory...
 - per-destination/origin load shedding

http stack test
 - support many virtual IPs in client server tests so we can exercise more ports.  Current max tested is 25k client -> 25k server.
 - pin threads / fe-be connection alignment / packet steering

server:
 - start as root, bind to ports/call initialize(), drop privs, call start()
 - readiness and liveness check support - e2e vs local options
 - plugin api

allocators:
 - add ctrs for discard allocator to track extra large chunk allocations
 - add counters to track stranded memory in discard allocators
 - create an option to disable all allocators - just forward to the source allocator unless you are system.  Then see if there are memory leaks and measure perf difference.  also measure perf difference vs. tcmalloc
 - Should placement new automatically set cleanup handler if object has one?  Put cleanup handler in ESB::Object?

containers:
 - lockless linked list
 - lockless hash table which uses lockless linked lists

cmake:
 - doxygen
 - code coverage
 - addr space randomization
 - more *san (undefined behavior detector, etc)

async dns client:
 - cares
 - start with caching sync impl and async interface.  cache according to the dns record ttl and LRU
 - common caching code.  intial async impl can use threadpool and gethostbyname

tcp proxy stuff:
 - make sure full duplex
 - track bps
 - turn into a unit test over loopback/same proc

http stuff
 - functions to hex encode/decode header fields
 - merge duplicate headers
 - add user agent/server agent headers
 - remove sensitive headers while proxying
 - fix double add of Transfer-Encoding: chunked.... strip headers that control connection lifetime during proxying
 - max requests per connection option (1 disables keepalives, 0 is unlimited)
 - max header size option
 - max body size option
 - save don't skip trailer
 - slow loris defense - not just idle time, also max transaction latency
 - h2 support, move parsing and formatting state from sockets to transactions
 - support 100 Continue - server side already supported?  client side - do after connection reuse?
 - support max requests per connection
 - support receiving half closed from client while still sending outstanding response
 - make all the unsigned chars in HttpMessage, HttpRequest, and HttpReponse chars?  UTF-8 only in encoded form?

URL 
 - canonicalization / encode and decode per https://url.spec.whatwg.org/
 - fully but lazily parse query string - if requested parse into a list of key value pairs.

bootstrap config
 - reload when file changes or on SIGHUP.  ports, buffer sizes, etc
 - xds agent could just regenerate this, serializing updates and using atomic moves
 - could potentially break into multiple files for scalability, but then need cross-file atomic update mechanism.
 - journalling approach?  base bootstrap file.  log of incremental updates.   compress incremental updates to avoid filling disk

logging
 - rotating file logger - batch into memory buffers and flush occasionally.  double buffer.  fill one while flush other.  if one to be filled is too small, drop messages and increment drop counter.
 - add trace id extension header, copy across all transactions, and include in log messages

metrics:
 - calc percentiles in latency - https://www.codeproject.com/Articles/25656/Calculating-Percentiles-in-Memory-bound-Applicatio
 - granular metrics for client and server sockets - break down failures by state and type (idle, error type)
 - metrics for TLS client and server errors
 - connection pool hit rate (averaging counter, add 1 for hit, 0 for miss)
